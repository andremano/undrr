<!DOCTYPE html>

<html :class="{'dark': darkMode === 'dark' || (darkMode === 'system' &amp;&amp; window.matchMedia('(prefers-color-scheme: dark)').matches)}" class="scroll-smooth" data-content_root="./" lang="en" x-data="{ darkMode: localStorage.getItem('darkMode') || localStorage.setItem('darkMode', 'system'), activeSection: '' }" x-init="$watch('darkMode', val =&gt; localStorage.setItem('darkMode', val))">
<head>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<meta charset="utf-8"/>
<meta content="white" media="(prefers-color-scheme: light)" name="theme-color"/>
<meta content="black" media="(prefers-color-scheme: dark)" name="theme-color"/>
<meta content="Docutils 0.18.1: http://docutils.sourceforge.net/" name="generator"/>
<title>Data for Disaster Risk Management | UNDRR 1 documentation</title>
<meta content="Data for Disaster Risk Management | UNDRR 1 documentation" property="og:title"/>
<meta content="Data for Disaster Risk Management | UNDRR 1 documentation" name="twitter:title"/>
<link href="_static/pygments.css?v=e72c8e07" rel="stylesheet" type="text/css"/>
<link href="_static/theme.css?v=42baaae4" rel="stylesheet" type="text/css"/>
<link href="_static/sphinx-design.min.css?v=95c83b7e" rel="stylesheet" type="text/css"/>
<link href="_static/itc_logo.css?v=fe299e43" rel="stylesheet" type="text/css"/>
<link href="_static/awesome-sphinx-design.css?v=15e0fffa" rel="stylesheet" type="text/css"/>
<link href="search.html" rel="search" title="Search"/>
<link href="genindex.html" rel="index" title="Index"/>
<link href="index.html" rel="prev" title="Welcome to undrr’s documentation!"/>
<script>
    <!-- Prevent Flash of wrong theme -->
      const userPreference = localStorage.getItem('darkMode');
      let mode;
      if (userPreference === 'dark' || window.matchMedia('(prefers-color-scheme: dark)').matches) {
        mode = 'dark';
        document.documentElement.classList.add('dark');
      } else {
        mode = 'light';
      }
      if (!userPreference) {localStorage.setItem('darkMode', mode)}
    </script>
</head>
<body :class="{ 'overflow-hidden': showSidebar }" class="min-h-screen font-sans antialiased bg-background text-foreground" x-data="{ showSidebar: false, showScrollTop: false }">
<div @click.self="showSidebar = false" class="fixed inset-0 z-50 overflow-hidden bg-background/80 backdrop-blur-sm md:hidden" x-cloak="" x-show="showSidebar"></div><div class="relative flex flex-col min-h-screen" id="page"><a class="absolute top-0 left-0 z-[100] block bg-background p-4 text-xl transition -translate-x-full opacity-0 focus:translate-x-0 focus:opacity-100" href="#content">
      Skip to content
    </a><header class="sticky top-0 z-40 w-full border-b shadow-sm border-border supports-backdrop-blur:bg-background/60 bg-background/95 backdrop-blur"><div class="container flex items-center h-14">
<div class="hidden mr-4 md:flex">
<a class="flex items-center mr-6" href="index.html">
<img alt="Logo" class="mr-2 dark:invert" height="24" src="_static/itc_logo.png" width="24"/><span class="hidden font-bold sm:inline-block text-clip whitespace-nowrap">UNDRR 1 documentation</span>
</a></div><button @click="showSidebar = true" class="inline-flex items-center justify-center h-10 px-0 py-2 mr-2 text-base font-medium transition-colors rounded-md hover:text-accent-foreground hover:bg-transparent md:hidden" type="button">
<svg aria-hidden="true" fill="currentColor" height="24" viewbox="0 96 960 960" width="24" xmlns="http://www.w3.org/2000/svg">
<path d="M152.587 825.087q-19.152 0-32.326-13.174t-13.174-32.326q0-19.152 13.174-32.326t32.326-13.174h440q19.152 0 32.326 13.174t13.174 32.326q0 19.152-13.174 32.326t-32.326 13.174h-440Zm0-203.587q-19.152 0-32.326-13.174T107.087 576q0-19.152 13.174-32.326t32.326-13.174h320q19.152 0 32.326 13.174T518.087 576q0 19.152-13.174 32.326T472.587 621.5h-320Zm0-203.587q-19.152 0-32.326-13.174t-13.174-32.326q0-19.152 13.174-32.326t32.326-13.174h440q19.152 0 32.326 13.174t13.174 32.326q0 19.152-13.174 32.326t-32.326 13.174h-440ZM708.913 576l112.174 112.174q12.674 12.674 12.674 31.826t-12.674 31.826Q808.413 764.5 789.261 764.5t-31.826-12.674l-144-144Q600 594.391 600 576t13.435-31.826l144-144q12.674-12.674 31.826-12.674t31.826 12.674q12.674 12.674 12.674 31.826t-12.674 31.826L708.913 576Z"></path>
</svg>
<span class="sr-only">Toggle navigation menu</span>
</button>
<div class="flex items-center justify-between flex-1 space-x-2 sm:space-x-4 md:justify-end">
<div class="flex-1 w-full md:w-auto md:flex-none"><form @keydown.k.window.meta="$refs.search.focus()" action="search.html" class="relative flex items-center group" id="searchbox" method="get">
<input aria-label="Search the docs" class="inline-flex items-center font-medium transition-colors bg-transparent focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 ring-offset-background border border-input hover:bg-accent focus:bg-accent hover:text-accent-foreground focus:text-accent-foreground hover:placeholder-accent-foreground py-2 px-4 relative h-9 w-full justify-start rounded-[0.5rem] text-sm text-muted-foreground sm:pr-12 md:w-40 lg:w-64" id="search-input" name="q" placeholder="Search ..." type="search" x-ref="search"/>
<kbd class="pointer-events-none absolute right-1.5 top-2 hidden h-5 select-none text-muted-foreground items-center gap-1 rounded border border-border bg-muted px-1.5 font-mono text-[10px] font-medium opacity-100 sm:flex group-hover:bg-accent group-hover:text-accent-foreground">
<span class="text-xs">⌘</span>
    K
  </kbd>
</form>
</div>
<nav class="flex items-center space-x-1">
<button @click="darkMode = darkMode === 'light' ? 'dark' : 'light'" aria-label="Color theme switcher" class="relative inline-flex items-center justify-center px-0 text-sm font-medium transition-colors rounded-md hover:bg-accent hover:text-accent-foreground h-9 w-9" type="button">
<svg class="absolute transition-all scale-100 rotate-0 dark:-rotate-90 dark:scale-0" fill="currentColor" height="24" viewbox="0 96 960 960" width="24" xmlns="http://www.w3.org/2000/svg">
<path d="M480 685q45.456 0 77.228-31.772Q589 621.456 589 576q0-45.456-31.772-77.228Q525.456 467 480 467q-45.456 0-77.228 31.772Q371 530.544 371 576q0 45.456 31.772 77.228Q434.544 685 480 685Zm0 91q-83 0-141.5-58.5T280 576q0-83 58.5-141.5T480 376q83 0 141.5 58.5T680 576q0 83-58.5 141.5T480 776ZM80 621.5q-19.152 0-32.326-13.174T34.5 576q0-19.152 13.174-32.326T80 530.5h80q19.152 0 32.326 13.174T205.5 576q0 19.152-13.174 32.326T160 621.5H80Zm720 0q-19.152 0-32.326-13.174T754.5 576q0-19.152 13.174-32.326T800 530.5h80q19.152 0 32.326 13.174T925.5 576q0 19.152-13.174 32.326T880 621.5h-80Zm-320-320q-19.152 0-32.326-13.174T434.5 256v-80q0-19.152 13.174-32.326T480 130.5q19.152 0 32.326 13.174T525.5 176v80q0 19.152-13.174 32.326T480 301.5Zm0 720q-19.152 0-32.326-13.17Q434.5 995.152 434.5 976v-80q0-19.152 13.174-32.326T480 850.5q19.152 0 32.326 13.174T525.5 896v80q0 19.152-13.174 32.33-13.174 13.17-32.326 13.17ZM222.174 382.065l-43-42Q165.5 327.391 166 308.239t13.174-33.065q13.435-13.674 32.587-13.674t32.065 13.674l42.239 43q12.674 13.435 12.555 31.706-.12 18.272-12.555 31.946-12.674 13.674-31.445 13.413-18.772-.261-32.446-13.174Zm494 494.761-42.239-43q-12.674-13.435-12.674-32.087t12.674-31.565Q686.609 756.5 705.38 757q18.772.5 32.446 13.174l43 41.761Q794.5 824.609 794 843.761t-13.174 33.065Q767.391 890.5 748.239 890.5t-32.065-13.674Zm-42-494.761Q660.5 369.391 661 350.62q.5-18.772 13.174-32.446l41.761-43Q728.609 261.5 747.761 262t33.065 13.174q13.674 13.435 13.674 32.587t-13.674 32.065l-43 42.239q-13.435 12.674-31.706 12.555-18.272-.12-31.946-12.555Zm-495 494.761Q165.5 863.391 165.5 844.239t13.674-32.065l43-42.239q13.435-12.674 32.087-12.674t31.565 12.674Q299.5 782.609 299 801.38q-.5 18.772-13.174 32.446l-41.761 43Q231.391 890.5 212.239 890t-33.065-13.174ZM480 576Z"></path>
</svg>
<svg class="absolute transition-all scale-0 rotate-90 dark:rotate-0 dark:scale-100" fill="currentColor" height="24" viewbox="0 96 960 960" width="24" xmlns="http://www.w3.org/2000/svg">
<path d="M480 936q-151 0-255.5-104.5T120 576q0-138 90-239.5T440 218q25-3 39 18t-1 44q-17 26-25.5 55t-8.5 61q0 90 63 153t153 63q31 0 61.5-9t54.5-25q21-14 43-1.5t19 39.5q-14 138-117.5 229T480 936Zm0-80q88 0 158-48.5T740 681q-20 5-40 8t-40 3q-123 0-209.5-86.5T364 396q0-20 3-40t8-40q-78 32-126.5 102T200 576q0 116 82 198t198 82Zm-10-270Z"></path>
</svg>
</button>
</nav>
</div>
</div>
</header>
<div class="flex-1"><div class="container flex-1 items-start md:grid md:grid-cols-[220px_minmax(0,1fr)] md:gap-6 lg:grid-cols-[240px_minmax(0,1fr)] lg:gap-10"><aside :aria-hidden="!showSidebar" :class="{ 'translate-x-0': showSidebar }" class="fixed inset-y-0 left-0 md:top-14 z-50 md:z-30 bg-background md:bg-transparent transition-all duration-100 -translate-x-full md:translate-x-0 ml-0 p-6 md:p-0 md:-ml-2 md:h-[calc(100vh-3.5rem)] w-5/6 md:w-full shrink-0 overflow-y-auto border-r border-border md:sticky" id="left-sidebar">
<a class="!justify-start text-sm md:!hidden bg-background" href="index.html">
<img alt="Logo" class="mr-2 dark:invert" height="16" src="_static/itc_logo.png" width="16"/><span class="font-bold text-clip whitespace-nowrap">UNDRR 1 documentation</span>
</a>
<div class="relative overflow-hidden md:overflow-auto my-4 md:my-0 h-[calc(100vh-8rem)] md:h-auto">
<div class="overflow-y-auto h-full w-full relative pr-6"><nav class="table w-full min-w-full my-6 lg:my-8">
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Data for Disaster Risk Management</a></li>
</ul>
</nav>
</div>
</div>
<button @click="showSidebar = false" class="absolute md:hidden right-4 top-4 rounded-sm opacity-70 transition-opacity hover:opacity-100" type="button">
<svg class="h-4 w-4" fill="currentColor" height="24" stroke="none" viewbox="0 96 960 960" width="24" xmlns="http://www.w3.org/2000/svg">
<path d="M480 632 284 828q-11 11-28 11t-28-11q-11-11-11-28t11-28l196-196-196-196q-11-11-11-28t11-28q11-11 28-11t28 11l196 196 196-196q11-11 28-11t28 11q11 11 11 28t-11 28L536 576l196 196q11 11 11 28t-11 28q-11 11-28 11t-28-11L480 632Z"></path>
</svg>
</button>
</aside>
<main class="relative py-6 lg:gap-10 lg:py-8 xl:grid xl:grid-cols-[1fr_300px]">
<div class="w-full min-w-0 mx-auto">
<nav aria-label="breadcrumbs" class="flex items-center mb-4 space-x-1 text-sm text-muted-foreground">
<a class="overflow-hidden text-ellipsis whitespace-nowrap hover:text-foreground" href="index.html">
<span class="hidden md:inline">UNDRR 1 documentation</span>
<svg aria-label="Home" class="md:hidden" fill="currentColor" height="18" stroke="none" viewbox="0 96 960 960" width="18" xmlns="http://www.w3.org/2000/svg">
<path d="M240 856h120V616h240v240h120V496L480 316 240 496v360Zm-80 80V456l320-240 320 240v480H520V696h-80v240H160Zm320-350Z"></path>
</svg>
</a>
<div class="mr-1">/</div><span aria-current="page" class="font-medium text-foreground overflow-hidden text-ellipsis whitespace-nowrap">Data for Disaster Risk Management</span>
</nav>
<div id="content" role="main">
<section id="data-for-disaster-risk-management">
<h1>Data for Disaster Risk Management<a class="headerlink" href="#data-for-disaster-risk-management" title="Link to this heading"><span>#</span></a></h1>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Overall objectives</p>
<p>After this session you should be able to:</p>
<ul class="simple">
<li><p>Recognize the variety of spatial data available for risk assessment and how different hazards require specific spatial, spectral, and temporal characteristics. Assess these characteristics across data types while considering additional constraints that impact data selection;</p></li>
<li><p>Understand what data sovereignty is and why it is important.</p></li>
</ul>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>What will you learn</p>
<p>In this session you will how learn to:</p>
<ul class="simple">
<li><p>Search and obtain key datasets;</p></li>
<li><p>Evaluate the quality of a dataset with regards to its suitability for a purpose;</p></li>
</ul>
</div>
<section id="what-is-spatial-data">
<h2>What is Spatial data?<a class="headerlink" href="#what-is-spatial-data" title="Link to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#what-is-spatial-data'"><span>#</span></a></h2>
<p>In geoinformatics, also called geoinformation science, we use the term <strong>spatial data</strong> to describe any type of data that can be linked to a geographic place, usually via coordinates. This means that spatial data has an <strong>unambiguous location</strong> (i.e. it can be associated to a specific location on the Planet). The classic data type is a map, a more modern one could be a satellite image (for an introduction on remote sensing see box). However, we need to consider that our work is largely done digitally on a computer, and that we might want to use data that are actually quite variable in nature. When we think about disasters or risk, we may want to include:</p>
<ul class="simple">
<li><p>Tabular data or statistics (e.g. on the number of hazard or disaster events of a certain type and in a given time period);</p></li>
<li><p>Thematic data (e.g. a road or river network, soil types, or digital elevation models [DEMs]);</p></li>
<li><p>Topographic maps;</p></li>
<li><p>Model results (e.g. for flood hazard or slope instability);</p></li>
<li><p>Images (e.g. aerial photos or satellite images).</p></li>
<li><p>Point cloud data (LIDAR and laser scans)</p></li>
</ul>
<p>In the next section we will take a closer look into different types of data.</p>
</section>
<section id="main-types-of-data">
<h2>Main types of data<a class="headerlink" href="#main-types-of-data" title="Link to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#main-types-of-data'"><span>#</span></a></h2>
<p>First, it is important to distinguish between <em>data type</em> and <em>data format</em>. The first usually refers to the nature of the data, that is what type of information is the data documenting, while the latter describes what type of computer file are you talking about, including technical specifications.</p>
<p>Regardless of the type and format, <em>data acquisition</em> is done through the use sensors, surveys and other methods, after which <em>data processing</em> follows in order to distil useful information. We will not do an exhaustive list of data acquisition and processing methods, still, <a class="reference internal" href="#data-type-data-format"><span class="std std-numref">Fig. 1</span></a> provides an overview of the most relevant data types for Disaster Risk Management.</p>
<figure class="align align-default" id="id4">
<span id="data-type-data-format"></span><img alt="Overview of the main data types" src="_images/data_type_data_format.png"/>
<figcaption>
<p><span class="caption-number">Fig. 1 </span><span class="caption-text">Overview of the main data types</span><a class="headerlink" href="#id4" title="Link to this image"><span>#</span></a></p>
</figcaption>
</figure>
<p>For example, you might find statistics presented in a table with either coordinates or grouped per administrative area, or illustrated as a chart or graphic. It can also happen that field photographs are available. Associating those with the other data, and integrating the information you think is useful in those photos with the rest of the analysis, can be challenging. Also consider that many maps or aerial photographs are available only as paper hardcopies. To use them in our work we first have to convert them to a digital format. This can be done by digitizing relevant information, or by scanning and subsequently georeferencing the maps or images.</p>
<p>Some of the data types mentioned in the diagram of <a class="reference internal" href="#data-type-data-format"><span class="std std-numref">Fig. 1</span></a> deserve a closer look due to their importance for Disaster Risk Management. Among all the data types, those acquired through <strong>remote sensing</strong> deserve especial attention - if you want to dive a bit more into what exactly it is remote sensing, we invite you to expand the <em>What is remote sensing</em> dropdown.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-m-5 sd-fade-in">
<summary class="sd-summary-title sd-card-header sd-bg-info sd-bg-text-info">
<span class="sd-summary-text">What is remote sensing?</span><span class="sd-summary-state-marker sd-summary-chevron-down"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-down" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M5.22 8.22a.749.749 0 0 0 0 1.06l6.25 6.25a.749.749 0 0 0 1.06 0l6.25-6.25a.749.749 0 1 0-1.06-1.06L12 13.939 6.28 8.22a.749.749 0 0 0-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">Remote sensing (RS) can be described as the process of making measurements or observations without direct contact with the object being measured or observed. Thus, while in the geoinformatics context satellites often come to mind, even amateur photography is a form of RS. It usually results in images, but also includes other measurements, such as of temperatures or gravity.</p>
<ul class="simple">
<li><p class="sd-card-text"><strong>Sensors and platforms</strong>. For remote sensing we normally require a <strong>sensor</strong> (i.e. a camera or scanner), but also something that carries the device. Such platforms can be airplanes or satellites, but also other instruments that allow us to place the sensor so that the area or object of interest is exposed, such as balloons or kites. The choice of platform directly affects what we can observe and how. Airplanes and helicopters are flexible in their operation, and by flying relatively low provide good spatial detail. However, such surveys can be expensive and regular imaging of the same area thus costly. Satellites fly on a fixed <strong>orbit</strong>, and are thus less flexible, but can provide data at regular intervals (think of trains on a track). We distinguish between so-called <strong>polar orbiters</strong>, whereby the satellites continuously circle the Earth at an altitude of some 500- 900km, passing over or near the poles. Normally only a relatively narrow strip of Earth underneath the sensor is observed. Modern satellites can also point the sensor sideways for greater flexibility. The other class of satellites is positioned in <strong>geostationary orbit</strong>. This means that the satellite is always directly above a designated place on the equator, moving with the rotating Earth at an altitude of 36,000 km. At that height the sensor can usually observe an entire hemisphere (the side of the Earth facing it), and provide data at any desired frequency. Many weather and communication satellites fall in this category, while most Earth observation satellites are polar orbiters.</p></li>
<li><p class="sd-card-text"><strong>Collecting information</strong>. The data we obtain depend primarily on the sensor type, just like you might take color or black/white photos with your camera. The secret to taking such different photos lies in the <strong>electromagnetic energy</strong> <a class="reference internal" href="#the-em-spectrum"><span class="std std-numref">Fig. 2</span></a>, which is what our sensors can detect. The most common source of energy is reflected sunlight, which, as you probably know, contains visible light, but also ultraviolet (UV), infrared (IR), thermal and other energy (Figure 2.1). Which part of this continuous energy band we capture depends on the sensor. Your camera might only capture visible light, while others can “see” UV, IR or thermal energy.</p></li>
</ul>
<figure class="align align-default" id="id5">
<span id="the-em-spectrum"></span><img alt="The EM spectrum" src="_images/the_em_spectrum.jpg"/>
<figcaption>
<p><span class="caption-number">Fig. 2 </span><span class="caption-text">The EM spectrum</span><a class="headerlink" href="#id5" title="Link to this image"><span>#</span></a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p class="sd-card-text"><strong>The data</strong>. The data our sensors record typically have the form of a grid, or raster. Rows and columns in that grid are populated by cells. These cells contain the information recorded by the sensor. A sensor can also have several <strong>bands</strong>, meaning that different sections of the electromagnetic spectrum are observed <a class="reference internal" href="#grid-structure"><span class="std std-numref">Fig. 3</span></a>. Thus for the area observed we will have an image that contains several bands, and the cell corresponding to a small part on the ground will have one data value for each band. The most important point to understand here is that different materials on the ground reflect energy in a characteristic spectral pattern. For example, vegetation is characterized by high energy in the near infrared (NIR), while for water the energy is very low. In figure 2.2 this would result in high values (digital numbers [DN]) for vegetation and low values for water in the band corresponding to the NIR.</p></li>
</ul>
<figure class="align align-default" id="id6">
<span id="grid-structure"></span><img alt="Grid structure of a multi-band image" src="_images/grid_structure.png"/>
<figcaption>
<p><span class="caption-number">Fig. 3 </span><span class="caption-text">Grid structure of a multi-band image</span><a class="headerlink" href="#id6" title="Link to this image"><span>#</span></a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p class="sd-card-text"><strong>Displaying an image</strong>. Once we have our data we can either display them directly on our monitor (if they are already digital), or first scan them. A monitor works with 3 different color channels (blue, green, red), and is able to generate any color (including black and white) with a combination of those 3 colors. Thus we can take an image with only 1 or with several bands and display 1 band at a time, thus as a <strong>pan-chromatic</strong> image <a class="reference internal" href="#image-visualizations"><span class="std std-numref">Fig. 4</span></a>. We can also use 3 bands and display them as a so- called <strong>true-color composite</strong> (B), which looks like the scene would look to us from space. However, we can essentially assign any of the image bands to one of the 3 colors. A typical combination, called a <strong>false-color composite</strong>, is shown in C, where the information from the  NIR band is displayed in red. Recall that vegetation leads to high DN values in the NIR, hence the high vegetation signal leads to a</p></li>
</ul>
<figure class="align align-default" id="id7">
<span id="image-visualizations"></span><img alt="A – panchromatic, B- true-color, C and D – false color composites" src="_images/image_visualizations.png"/>
<figcaption>
<p><span class="caption-number">Fig. 4 </span><span class="caption-text">A – panchromatic, B- true-color, C and D – false color composites</span><a class="headerlink" href="#id7" title="Link to this image"><span>#</span></a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p class="sd-card-text"><strong>Enhancing an image</strong>. Sometimes, for information to be made more visible, we have to enhance the image. One typical form is <strong>stretching</strong>. Our displays are typically able to display 256 brightness levels for each color, corresponding to 8bit. However, very often the image data only have a limited range, say with DNs between 50 and 150, where are not very bright or very dark features on the ground. To achieve a display with a richer contrast we can stretch the data over the entire available range (0-255). The same concept applies to other data types you will work with, for example elevation. The elevation file for our test area ranges between approximately 900 and 1350m. By default they will be stretched over the available display range. However, we can also stretch a small value range, say 950-1000, to highlight more details. Another common enhancing method is <strong>filtering</strong> <a class="reference internal" href="#filtering"><span class="std std-numref">Fig. 5</span></a>. This is a so-called neighborhood analysis, often used to smoothen an image or to highlight edges. In the example the average of all cells shown in grey in the input image is calculated and written to a new file, before the filter template moves to the next pixel (hatched box). Many filter types have been developed, which you will also use in the ILWIS exercises (for example shadow and smoothing filters).</p></li>
</ul>
<blockquote>
<div><figure class="align align-default" id="id8">
<span id="filtering"></span><img alt="Input and output result of filtering:  In this case, a smoothing filter was applied." src="_images/filtering.jpg"/>
<figcaption>
<p><span class="caption-number">Fig. 5 </span><span class="caption-text">Input and output result of filtering:  In this case, a smoothing filter was applied.</span><a class="headerlink" href="#id8" title="Link to this image"><span>#</span></a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p class="sd-card-text"><strong>Other factors influencing our data</strong>. RS data come in many forms, often described by <strong>sensor type</strong>, as well as <strong>spatial, temporal</strong> and <strong>spectral resolution</strong>. Sensors recording reflected sunlight or energy emitted by the earth are called <strong>passive sensors</strong>. However, we also have sensors that emit their own energy, which is reflected by the earth, just like you use a flash on your camera. These are <strong>active sensors</strong>, well-known examples being radar (see Figure 2.10) or laser scanning. The <strong>spatial resolution</strong> describes the size of the ground area represented in a single pixel. This largely depends on the distance between the sensor and the object. While aerial photos may have a resolution of a few cm, data from polar orbiters range between about 50 cm and 1 km per cell. Sensors on geostationary satellites, being very far away, record data at resolutions of a few km. The <strong>temporal</strong> resolution describes the possible frequency of repeat observations. For aerial surveys this can be years. Depending on the type of polar orbiter and sensor, their temporal resolution varies between approx. 1 and 44 days, while geostationary sensors record data up to every 15 minutes. The <strong>spectral</strong> resolution describes how narrow a slice of the EM spectrum a sensor band records.</p></li>
</ul>
</div></blockquote>
</div>
</details><section id="digital-elevation-models-dem">
<h3>Digital Elevation Models (DEM)<a class="headerlink" href="#digital-elevation-models-dem" title="Link to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#digital-elevation-models-dem'"><span>#</span></a></h3>
<p><strong>Digital Elevation Models (DEM)</strong>  consist of a single band image where the pixel value represents the elevation of that location <a class="reference internal" href="#bala-savalan-peak-iran-dem-srtm"><span class="std std-numref">Fig. 6</span></a>. They are a fundamental and indispensable dataset for many applications because there are many other informations that can be derived from it, especially when it comes to hydrology. In fact, delineation of catchment areas, streams, flood simulations cannot be done without a DEM as input.</p>
<figure class="align align-default" id="id9">
<span id="bala-savalan-peak-iran-dem-srtm"></span><img alt="DEM of the Savalan Peak (Iran) based on SRTM data" src="_images/bala_savalan_peak_%28Iran%29_DEM_srtm.png"/>
<figcaption>
<p><span class="caption-number">Fig. 6 </span><span class="caption-text">DEM of the Savalan Peak (Iran) based on SRTM data</span><a class="headerlink" href="#id9" title="Link to this image"><span>#</span></a></p>
</figcaption>
</figure>
<p>DEM are also essential for all sort of landscape analysis for their unique ability to provide an intuitive reading on the main features of an area: where are the mountains, the valleys, the flat areas and so on <a class="reference internal" href="#dem-animation"><span class="std std-numref">Fig. 7</span></a>:</p>
<figure class="align align-default" id="id10">
<span id="dem-animation"></span><img alt="3D visualization of the DEM of the Savalan Peak (Iran) based on SRTM data" src="_images/dem_animation.gif"/>
<figcaption>
<p><span class="caption-number">Fig. 7 </span><span class="caption-text">3D visualization of the DEM of the Savalan Peak (Iran) based on SRTM data</span><a class="headerlink" href="#id10" title="Link to this image"><span>#</span></a></p>
</figcaption>
</figure>
</section>
<section id="land-cover-maps">
<h3>Land Cover Maps<a class="headerlink" href="#land-cover-maps" title="Link to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#land-cover-maps'"><span>#</span></a></h3>
<p>Land Cover maps are a form of thematic data where the map is made of mutually exclusive categories that are defined according to the prevalent land cover.
For example, a land cover map with four categories could include <em>water</em>, <em>green area</em>, <em>dry area</em>, and <em>urbanized</em>. Land cover maps are often confused with land use maps, and the two terms are mistakenly used interchangeably. Land cover refers to the actual physical surface of an area—what dominates the landscape :numref:land_cover_enschede.
In contrast, land use maps document how people utilize the land. For instance, green area describes the land cover, but park is a land use category, not a cover type.</p>
<figure class="align align-default" id="id11">
<span id="land-cover-enschede"></span><img alt="Land cover map of Enschede (The Netherlands) based on Sentinel 2 imagery (2016)" src="_images/land_cover_enschede.png"/>
<figcaption>
<p><span class="caption-number">Fig. 8 </span><span class="caption-text">Land cover map of Enschede (The Netherlands) based on Sentinel 2 imagery (2016)</span><a class="headerlink" href="#id11" title="Link to this image"><span>#</span></a></p>
</figcaption>
</figure>
<p>Land cover maps are typically produced by classifying multi-spectral satellite imagery using a range of machine learning and supervised classification techniques. These methods aim to cluster pixels based on radiometric similarity. The accuracy of the classification is then assessed by evaluating whether the assigned categories correctly match the actual land cover.
The more localized a land cover map is, the more accurate and representative the land cover classes tend to be. However, there exists land cover maps at global scale that might be useful even when used for large scale mapping. See for example <a class="reference external" href="https://esa-worldcover.org/en">Worldwide land cover mapping</a></p>
</section>
<section id="land-cover-indices">
<h3>Land Cover Indices<a class="headerlink" href="#land-cover-indices" title="Link to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#land-cover-indices'"><span>#</span></a></h3>
<p>Land Cover Indices are derived from remotely sensed data, primarily multi-spectral satellite imagery, and are expressed on a numerical scale, typically ranging from -1 to 1. Higher values indicate a greater likelihood that the physical characteristic measured by the index is present. These indices are widely used in environmental monitoring, agriculture, urban planning and may also be very useful for disaster risk managing. Indices allow us to analyze vegetation, water bodies, soil, and built-up areas.
A few of these indices are very commonly used:</p>
<ul class="simple">
<li><p>Normalized Difference Vegetation Index (NDVI) – Measures vegetation health and density. Defined as:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[NDVI = \frac{(NIR - RED)}{(NIR + RED)}\]</div>
<ul class="simple">
<li><p>Normalized Difference Water Index (NDWI) – Indicates the presence of water on the surface (water bodies).</p></li>
</ul>
<div class="math notranslate nohighlight">
\[NDWI = \frac{(GREEN - NIR)}{(GREEN + NIR)}\]</div>
<p>Representing physical characteristics as an indice is a very useful indicator that also allows for a fast and intuitive assessment of complex phenomenon <a class="reference internal" href="#indeces-ndvi-ndwi"><span class="std std-numref">Fig. 9</span></a>. There are many indices built on top of remote sensed imagery, you can check this page for a <a class="reference external" href="https://github.com/awesome-spectral-indices/awesome-spectral-indices">list of indices</a></p>
<figure class="align align-default" id="id12">
<span id="indeces-ndvi-ndwi"></span><img alt="NDVI (A) and NDWI (B) indeces for the Sistan Basin, in Iran, as of January 2005." src="_images/indeces_ndvi_ndwi.png"/>
<figcaption>
<p><span class="caption-number">Fig. 9 </span><span class="caption-text">NDVI (A) and NDWI (B) indeces for the Sistan Basin, in Iran, as of January 2005.</span><a class="headerlink" href="#id12" title="Link to this image"><span>#</span></a></p>
</figcaption>
</figure>
</section>
<section id="aerial-and-drone-photography">
<h3>Aerial and drone photography<a class="headerlink" href="#aerial-and-drone-photography" title="Link to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#aerial-and-drone-photography'"><span>#</span></a></h3>
<p>Despite the increasing availability and quality of satellite imagery, mounting a photographic camera on an aerial vehicle is widely used and covers use cases for which satellite imagery is not the best option. In simple terms, if small object recognition is a requirement, then we need imagey that is suitable for small scale mapping.
The biggest difference between satellite and aerial and drone imagery is the spatial resolution that is higher in the latter <a class="reference internal" href="#satellite-arerial-drone-imagery"><span class="std std-numref">Fig. 10</span></a>. Commercial satellites offer spacial resolutions, for True colour of up to 3m while with aerial and drone photography we can have imagery with centimetric spatial resolution.</p>
<figure class="align align-default" id="id13">
<span id="satellite-arerial-drone-imagery"></span><img alt="Imagery documenting Caldas da Rainha, Portugal: (A) Sentinel-2 satellite imagery with a spatial resolution of 10m; (B) a highlighted section of the city captured in aerial photography with a resolution of 10cm; (C) the same highlighted section using drone imagery at 2cm resolution. Notice how the detail increases." src="_images/satellite_arerial_drone_imagery.png"/>
<figcaption>
<p><span class="caption-number">Fig. 10 </span><span class="caption-text">Imagery documenting Caldas da Rainha, Portugal: (A) Sentinel-2 satellite imagery with a spatial resolution of 10m; (B) a highlighted section of the city captured in aerial photography with a resolution of 10cm; (C) the same highlighted section using drone imagery at 2cm resolution. Notice how the detail increases.</span><a class="headerlink" href="#id13" title="Link to this image"><span>#</span></a></p>
</figcaption>
</figure>
<p>Aerial photography is usually comissioned by national agencies to obtain a detailed based map for the whole country or a particular region, while drone imagery, due to the logistic challenge of scaling it up to large areas, is usually applied to cover localized areas like a development plan or a particular part of a city.</p>
</section>
<section id="radar-data">
<h3>Radar data<a class="headerlink" href="#radar-data" title="Link to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#radar-data'"><span>#</span></a></h3>
<p>Radar data is obtained from active remote sensing satellites that emit energy waves in the microwave region of the electromagnetic spectrum toward the Earth’s surface. These waves then bounce back after interacting with the surface. By analyzing the time delay and characteristics of the returning signal, it is possible to distinguish surface features and objects once the data is processed into an image.</p>
<p>The main advantage of radar data—such as that provided by Synthetic Aperture Radar (SAR) sensors onboard the Sentinel-1 satellites—is that meteorological conditions like cloud cover do not affect data quality. This is in contrast to optical sensors, which depend on clear skies to capture usable imagery. Therefore, radar is an extremely valuable tool for near real-time monitoring of hazards commonly associated with cloud coverage, such as floods <a class="reference internal" href="#radar-imagery"><span class="std std-numref">Fig. 11</span></a>.</p>
<figure class="align align-default" id="id14">
<span id="radar-imagery"></span><img alt="Example of radar imagery showing water (depicted by dark pixels). The image compares the same region in April 2016 (pre-flood event) to the peak of the flood in August 2016. Cloud coverage did not impede the acquisition of useful imagery." src="_images/radar_imagery.png"/>
<figcaption>
<p><span class="caption-number">Fig. 11 </span><span class="caption-text">Example of radar imagery showing water (depicted by dark pixels). The image compares the same region in April 2016 (pre-flood event) to the peak of the flood in August 2016. Cloud coverage did not impede the acquisition of useful imagery.</span><a class="headerlink" href="#id14" title="Link to this image"><span>#</span></a></p>
</figcaption>
</figure>
</section>
<section id="lidar-data">
<h3>LiDAR data<a class="headerlink" href="#lidar-data" title="Link to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#lidar-data'"><span>#</span></a></h3>
<p>LiDAR or <em>Light Detection and Ranging</em> is also an active remote sensing system that can be used to generate very high resolution (in other words, detailed) Digital Elevation and Digital Surface Models <a class="reference internal" href="#lidar"><span class="std std-numref">Fig. 13</span></a></p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-m-5 sd-fade-in">
<summary class="sd-summary-title sd-card-header sd-bg-info sd-bg-text-info">
<span class="sd-summary-text">Difference between DEM and DSM</span><span class="sd-summary-state-marker sd-summary-chevron-down"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-down" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M5.22 8.22a.749.749 0 0 0 0 1.06l6.25 6.25a.749.749 0 0 0 1.06 0l6.25-6.25a.749.749 0 1 0-1.06-1.06L12 13.939 6.28 8.22a.749.749 0 0 0-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">Although DEM (Digital Elevation Models) and DSM (Digital Surface Models) are often mentioned interchangeably, they have slightly different meaning:</p>
<ul class="simple">
<li><p class="sd-card-text"><strong>DEM</strong> Is a representation of the topography without any other features like constructions or trees. It represents the height of ‘bare earth’ only <a class="reference internal" href="#dem-vs-dsm"><span class="std std-numref">Fig. 12</span></a>.</p></li>
<li><p class="sd-card-text"><strong>DSM</strong> In turn, is a representation of the topography that includes features that are on the ‘bare soil’ like houses and vegetation <a class="reference internal" href="#dem-vs-dsm"><span class="std std-numref">Fig. 12</span></a>.</p></li>
</ul>
<figure class="align align-default" id="id15">
<span id="dem-vs-dsm"></span><img alt="Same area as a DEM (A) and DSM (B). Note how the DSM is representing the top of the trees and that reflects in the elevation values." src="_images/dem_vs_dsm.png"/>
<figcaption>
<p><span class="caption-number">Fig. 12 </span><span class="caption-text">Same area as a DEM (A) and DSM (B). Note how the DSM is representing the top of the trees and that reflects in the elevation values</span><a class="headerlink" href="#id15" title="Link to this image"><span>#</span></a></p>
</figcaption>
</figure>
</div>
</details><figure class="align align-default" id="id16">
<span id="lidar"></span><img alt="DSM of the Neštich hillfort above Svätý Jur (Slovenia) made from LiDAR data (2016)" src="_images/lidar.gif"/>
<figcaption>
<p><span class="caption-number">Fig. 13 </span><span class="caption-text">DSM of the Neštich hillfort above Svätý Jur (Slovenia) made from LiDAR data (2016)</span><a class="headerlink" href="#id16" title="Link to this image"><span>#</span></a></p>
</figcaption>
</figure>
<p>LiDAR data acquisition is performed using a laser beam, typically mounted on an aircraft. The laser “fires” pulses toward the Earth’s surface, with point densities typically ranging from 1 to 100 points per square meter. Higher point densities provide more detailed data but also require more intensive processing.</p>
<p>The collected data forms a point cloud consisting of millions of points, each represented by XYZ coordinates. These coordinates are determined by measuring the time it takes for the laser beam to reach an object and reflect back to the sensor.</p>
<p>One particularly interesting application of high-density point clouds is their ability to penetrate vegetation and capture multiple layers of information. This allows for the identification of different levels, such as bare soil, intermediate vegetation, and the top of the tree canopy <a class="reference internal" href="#lidar-flying"><span class="std std-numref">Fig. 14</span></a>.</p>
<blockquote>
<div><figure class="align align-default" id="id17">
<span id="lidar-flying"></span><img alt="Animation illustrating the level of detail collected with a LiDAR sensor mounted on an airplane." src="_images/lidar_flying.gif"/>
<figcaption>
<p><span class="caption-number">Fig. 14 </span><span class="caption-text">Animation illustrating the level of detail collected with a LiDAR sensor mounted on an airplane</span><a class="headerlink" href="#id17" title="Link to this image"><span>#</span></a></p>
</figcaption>
</figure>
</div></blockquote>
</section>
<section id="topographic-maps">
<h3>Topographic maps<a class="headerlink" href="#topographic-maps" title="Link to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#topographic-maps'"><span>#</span></a></h3>
<p>Topographic maps are the result of surveys using traditional optical survey methods like total stations and theodolites, but nowadays, these surveys are typically assisted by GPS measurements and can be complemented with other aerial imagery or even LiDAR and other sensors.
Topographic maps include two big groups of information: man-made structures like roads and buildings and natural features with a great emphasis on altitude measurements that are on the base of terain representations and the deliniation of landscape features like ridges, valleys and water bodies <a class="reference internal" href="#topographic-map-example"><span class="std std-numref">Fig. 15</span></a></p>
<figure class="align align-default" id="id18">
<span id="topographic-map-example"></span><img alt='Detail of a topographic map the Tehachapi Mountains (California, USA). Section of "The National Map" by USGS' src="_images/topographic_map_example.png"/>
<figcaption>
<p><span class="caption-number">Fig. 15 </span><span class="caption-text">Detail of a topographic map the Tehachapi Mountains (California, USA). Section of “The National Map” by USGS</span><a class="headerlink" href="#id18" title="Link to this image"><span>#</span></a></p>
</figcaption>
</figure>
<p>Modern topographic maps are actually a composition of several distinct datasets that were acquired using a myriad of different techniques, however in many countries old paper based topographic maps continue to be a precious source of information.
When a map is made of a a subset of of topographic elements in order to document a specific theme, we call it a ‘thematic map’. Common thematic maps include natural features like geology <a class="reference internal" href="#geology-map"><span class="std std-numref">Fig. 16</span></a> or man-made elements like communications and cadastral maps (or other delimitations) <a class="reference internal" href="#cadastral-map-dorset"><span class="std std-numref">Fig. 17</span></a>.</p>
<figure class="align align-default" id="id19">
<span id="geology-map"></span><img alt="Thematic map of the geology from the peninsula of Peniche (Portugal)" src="_images/geology_map.png"/>
<figcaption>
<p><span class="caption-number">Fig. 16 </span><span class="caption-text">Thematic map of the geology from the peninsula of Peniche (Portugal)</span><a class="headerlink" href="#id19" title="Link to this image"><span>#</span></a></p>
</figcaption>
</figure>
<figure class="align align-default" id="id20">
<span id="cadastral-map-dorset"></span><img alt="Thematic map of the land parcels and roads from Dorset (Tasmania)" src="_images/cadastral_map_dorset.png"/>
<figcaption>
<p><span class="caption-number">Fig. 17 </span><span class="caption-text">Thematic map of the land parcels and roads from Dorset (Tasmania)</span><a class="headerlink" href="#id20" title="Link to this image"><span>#</span></a></p>
</figcaption>
</figure>
</section>
<section id="volunteered-geographic-information-vgi">
<h3>Volunteered geographic information (VGI)<a class="headerlink" href="#volunteered-geographic-information-vgi" title="Link to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#volunteered-geographic-information-vgi'"><span>#</span></a></h3>
<p>Volunteered geographic information (VGI) is a term used to denote spatial data is produced by volunteers with the explicit purpose of contributing to larger mapping project. The information produced this way is usually used as an addition or improvement to a reference map or as an element depicting a particular event for which the volunteers were mobilized such as campaings to map affected areas after an hazard.
The most well-known example is <a class="reference external" href="https://www.openstreetmap.org/">Open Street Map (OSM)</a> <a class="reference internal" href="#osm-nairobi"><span class="std std-numref">Fig. 18</span></a>, a project initiated in 2006 with the explicit objective of building a map of the entire world under a permissive use license. Nowadays, OSM does not rely solely on data produced by volunteers, but the OSM mapping community continues to play a crucial role—especially in data-poor contexts.</p>
<figure class="align align-default" id="id21">
<span id="osm-nairobi"></span><img alt="Map of Nairobi (Kenia) at the OSM portal" src="_images/osm_nairobi.png"/>
<figcaption>
<p><span class="caption-number">Fig. 18 </span><span class="caption-text">Map of Nairobi (Kenia) at the OSM portal</span><a class="headerlink" href="#id21" title="Link to this image"><span>#</span></a></p>
</figcaption>
</figure>
</section>
<section id="social-media-and-media-repositories-apis">
<h3>Social media and media repositories APIs<a class="headerlink" href="#social-media-and-media-repositories-apis" title="Link to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#social-media-and-media-repositories-apis'"><span>#</span></a></h3>
<p>Another interesting source of data is that produced in the context of social media applications. Most social networks provide an Application Programming Interface (API), which is essentially a collection of methods that allow for exploring and retrieving (anonymized) data published on a social networking platform.
These services are usually behind a paid subscription or offer very limited functionality in their free versions, but they can be a valuable source of data for investigating people’s behavior before, during, and after a disaster.
For example, researchers have used Twitter data to understand the extent to which citizens of Jakarta, Indonesia, utilize government-designated shelter facilities during a flood, as opposed to seeking shelter with family, friends, or other (perceived) safe locations <a class="reference internal" href="#jakarta-twitter"><span class="std std-numref">Fig. 19</span></a>.</p>
<figure class="align align-default" id="id22">
<span id="jakarta-twitter"></span><img alt="Type of shelters sought by citizens during the 2014 flood in Jakarta (excerpt) based on Twitter data. Adapted from da Silva Mano (2018)" src="_images/jakarta_twitter.png"/>
<figcaption>
<p><span class="caption-number">Fig. 19 </span><span class="caption-text">Type of shelters sought by citizens during the 2014 flood in Jakarta (excerpt) based on Twitter data. Adapted from da Silva Mano (2018) <a class="footnote-reference brackets" href="#id2" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>.</span><a class="headerlink" href="#id22" title="Link to this image"><span>#</span></a></p>
</figcaption>
</figure>
</section>
<section id="census-and-statistical-data">
<h3>Census and statistical data<a class="headerlink" href="#census-and-statistical-data" title="Link to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#census-and-statistical-data'"><span>#</span></a></h3>
<p>Census data is information about a population. Based on it, a series of statistical indicators can be established to construct a socio-economic profile of the population of a country, a region, or a city. Historically, population censuses aimed to count the number of inhabitants to allow more efficient tax collection and military drafting.
Nowadays, population censuses are much more than that and are an essential source of information for planning prevention and mitigation policies <a class="reference internal" href="#census-data"><span class="std std-numref">Fig. 20</span></a>.</p>
<figure class="align align-default" id="id23">
<span id="census-data"></span><img alt="Map showing population change from 2010 to 2020, based on data from the 2010 and 2020 censuses. Accurate population counts are essential to reliably measure these changes." src="_images/census_data.png"/>
<figcaption>
<p><span class="caption-number">Fig. 20 </span><span class="caption-text">Map showing population change from 2010 to 2020, based on data from the 2010 and 2020 censuses. Accurate population counts are essential to reliably measure these changes.</span><a class="headerlink" href="#id23" title="Link to this image"><span>#</span></a></p>
</figcaption>
</figure>
<p>Population data is typically collected by national agencies with a specific mandate, often starting at the household level and then aggregated into larger units such as neighborhoods, districts, or regions. However, due to a range of complex challenges—such as limited resources, logistical constraints, or political instability—not all countries are able to conduct systematic population surveys at the household level. In such cases, estimates based on larger aggregation units serve as the best available proxy for understanding population distribution.
A good example of this approach is the  <a class="reference external" href="https://hub.worldpop.org/">WorldPop</a> project. WorldPop provides gridded population datasets derived from a combination of census data, satellite imagery, and statistical modeling, offering valuable insights for countries where detailed household-level data is unavailable <a class="reference internal" href="#world-pop"><span class="std std-numref">Fig. 21</span></a>.</p>
<figure class="align align-default" id="id24">
<span id="world-pop"></span><img alt="A visualization of WorldPop population estimates (100x100m grid cells) for Iku Island, Lake Kivu (Democratic Republic of the Congo), as of 2020" src="_images/world_pop.png"/>
<figcaption>
<p><span class="caption-number">Fig. 21 </span><span class="caption-text">A visualization of WorldPop population estimates (100x100m grid cells) for Iku Island, Lake Kivu (Democratic Republic of the Congo), as of 2020.</span><a class="headerlink" href="#id24" title="Link to this image"><span>#</span></a></p>
</figcaption>
</figure>
</section>
<section id="sensor-data">
<h3>Sensor data<a class="headerlink" href="#sensor-data" title="Link to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#sensor-data'"><span>#</span></a></h3>
<p>In an increasingly connected world, the ability to collect and relay data in near real-time is becoming ever more feasible. This is achieved through a vast array of sensors that transmit readings via networks—most commonly over the internet using the HTTPS protocol. These sensors come in many forms and can be used to monitor human activities, such as traffic volumes, or more frequently, environmental indicators like temperature, air quality, wind speed, and more <a class="reference internal" href="#sensor-zanzibar"><span class="std std-numref">Fig. 22</span></a>.</p>
<figure class="align align-default" id="id25">
<span id="sensor-zanzibar"></span><img alt="Example of air temperature data collected by a sensor installed in Zanzibar, Tanzania. The data is visualized using the OpenSensorWeb portal." src="_images/zanzibar_sensor.png"/>
<figcaption>
<p><span class="caption-number">Fig. 22 </span><span class="caption-text">Example of air temperature data collected by a sensor installed in Zanzibar, Tanzania. The data is visualized using the OpenSensorWeb portal.</span><a class="headerlink" href="#id25" title="Link to this image"><span>#</span></a></p>
</figcaption>
</figure>
<p>The data collected by sensors can be used to feed data portals, such as the one shown in <a class="reference internal" href="#sensor-zanzibar"><span class="std std-numref">Fig. 22</span></a>, but it can also be distributed in raw format—either via APIs or through direct downloads in tabular formats <a class="reference internal" href="#zanzibar-sensor-data"><span class="std std-numref">Fig. 23</span></a>.</p>
<figure class="align align-default" id="id26">
<span id="zanzibar-sensor-data"></span><img alt="The the data behind the visualization in :numref:`sensor_zanzibar` but as a table, suitable for using with third party applications." src="_images/zanzibar_sensor_data.png"/>
<figcaption>
<p><span class="caption-number">Fig. 23 </span><span class="caption-text">The the data behind the visualization in <a class="reference internal" href="#sensor-zanzibar"><span class="std std-numref">Fig. 22</span></a> but as a table, suitable for using with third party applications.</span><a class="headerlink" href="#id26" title="Link to this image"><span>#</span></a></p>
</figcaption>
</figure>
</section>
</section>
<section id="sources-of-data">
<h2>Sources of data<a class="headerlink" href="#sources-of-data" title="Link to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#sources-of-data'"><span>#</span></a></h2>
<p>Most of the data types described in the previous section are made available through data portals. These portals may operate at regional, national, continental, or global scales, depending on the mandate and policies of the responsible institutions.</p>
<p>In the table below, we provide a reference list of data portals with global or continental scope. While the list is not exhaustive, it includes essential resources—platforms that are also highly valuable for work at national or local levels.</p>
<table class="docutils align-default" id="data-portals-table">
<caption><span class="caption-number">Table 1 </span><span class="caption-text">Key Global and Continental Data Portals</span><a class="headerlink" href="#data-portals-table" title="Link to this table"><span>#</span></a></caption>
<colgroup>
<col style="width: 20%"/>
<col style="width: 35%"/>
<col style="width: 45%"/>
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Resource</p></th>
<th class="head"><p>Link</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Open Street Map</p></td>
<td><p><a class="reference external" href="https://www.openstreetmap.org/">Open Street Map</a></p></td>
<td><p>Crowdsourced global basemap and vector data</p></td>
</tr>
<tr class="row-odd"><td><p>WorldPop</p></td>
<td><p><a class="reference external" href="https://hub.worldpop.org/">WorldPop</a></p></td>
<td><p>High-resolution population and demographic datasets</p></td>
</tr>
<tr class="row-even"><td><p>NORA</p></td>
<td><p><a class="reference external" href="https://www.ncei.noaa.gov/products/natural-hazards">NORA</a></p></td>
<td><p>NOAA archive for natural hazard data</p></td>
</tr>
<tr class="row-odd"><td><p>NASA Earth Data Search</p></td>
<td><p><a class="reference external" href="https://search.earthdata.nasa.gov/search">NASA Earth Data</a></p></td>
<td><p>Search and access NASA Earth observation data</p></td>
</tr>
<tr class="row-even"><td><p>USGS Earth Explorer</p></td>
<td><p><a class="reference external" href="https://earthexplorer.usgs.gov/">Earth Explorer</a></p></td>
<td><p>USGS platform for satellite and aerial imagery</p></td>
</tr>
<tr class="row-odd"><td><p>Copernicus Browser</p></td>
<td><p><a class="reference external" href="https://browser.dataspace.copernicus.eu/">Copernicus Browser</a></p></td>
<td><p>Browse and download Sentinel satellite data</p></td>
</tr>
<tr class="row-even"><td><p>Google Earth Engine</p></td>
<td><p><a class="reference external" href="https://earthengine.google.com/">Earth Engine</a></p></td>
<td><p>Cloud-based geospatial analysis platform</p></td>
</tr>
<tr class="row-odd"><td><p>OpenTopography</p></td>
<td><p><a class="reference external" href="https://opentopography.org/">OpenTopography</a></p></td>
<td><p>Access to global topographic and LiDAR data</p></td>
</tr>
<tr class="row-even"><td><p>Open Sensor Web</p></td>
<td><p><a class="reference external" href="https://www.opensensorweb.de/en/">Open Sensor Web</a></p></td>
<td><p>Real-time environmental sensor data viewer</p></td>
</tr>
<tr class="row-odd"><td><p>WorldPop Hub</p></td>
<td><p><a class="reference external" href="https://hub.worldpop.org/">WorldPop Hub</a></p></td>
<td><p>Population datasets and tools by WorldPop</p></td>
</tr>
<tr class="row-even"><td><p>Overture Maps</p></td>
<td><p><a class="reference external" href="https://overturemaps.org/">Overture Maps</a></p></td>
<td><p>Open mapping data by tech industry alliance</p></td>
</tr>
<tr class="row-odd"><td><p>ESA WorldCover</p></td>
<td><p><a class="reference external" href="https://esa-worldcover.org/en">WorldCover Mapping</a></p></td>
<td><p>Global land cover map from ESA</p></td>
</tr>
<tr class="row-even"><td><p>GeoNames</p></td>
<td><p><a class="reference external" href="https://geonames.org">GeoNames</a></p></td>
<td><p>Global database of geographic names</p></td>
</tr>
<tr class="row-odd"><td><p>Natural Earth</p></td>
<td><p><a class="reference external" href="https://www.naturalearthdata.com/">Natural Earth</a></p></td>
<td><p>Public domain map data for cartography</p></td>
</tr>
<tr class="row-even"><td><p>Database of Global Administrative Areas</p></td>
<td><p><a class="reference external" href="https://gadm.org/">GADM</a></p></td>
<td><p>Country and regional boundary maps worldwide</p></td>
</tr>
</tbody>
</table>
<p>Large data portals, such as those operated by NASA or the United States Geological Survey (USGS), provide access to a wide range of satellite imagery, including global Digital Elevation Models (DEMs) like SRTM and ASTER.
To make the most of these resources, it is important to understand the key characteristics of the sensors used to acquire the data, enabling more informed decision-making. Catalogs of sensor specifications, such as  <a class="reference external" href="https://webapps.itc.utwente.nl/sensor/default.aspx?view=allsatellites">the one provided by ITC</a>, can also serve as a valuable reference.</p>
</section>
<section id="data-quality">
<h2>Data quality<a class="headerlink" href="#data-quality" title="Link to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#data-quality'"><span>#</span></a></h2>
<p>Data quality is often misunderstood as being synonymous with <em>perfect data</em>. While this assumption is understandable, it’s not a constructive way to think about what <em>data quality</em> really means.</p>
<p>A more useful approach is to define data quality in terms of <strong>fitness for purpose</strong>. In other words, a dataset may be perfectly suitable for one application but inadequate for another. Assessing data quality, therefore, requires us to consider the context in which the data will be used.</p>
<section id="assessing-data-quality">
<h3>Assessing data quality<a class="headerlink" href="#assessing-data-quality" title="Link to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#assessing-data-quality'"><span>#</span></a></h3>
<p>To evaluate whether a (spatial) dataset meets the necessary quality standards, we first need to define the <em>ideal</em> requirements. This can be done by answering a few guiding questions:</p>
<ul class="simple">
<li><p><strong>What is the scale of the analysis?</strong> → related to <em>scope</em></p></li>
<li><p><strong>What level of detail is needed?</strong> → relates to <em>granularity</em> and <em>accuracy</em></p></li>
<li><p><strong>What type of information is required?</strong> → relates to <em>completeness</em></p></li>
<li><p><strong>How current or frequent must the data be?</strong> → relates to <em>temporal resolution</em></p></li>
</ul>
<p>Once we have clear answers to these questions, we can compare them with the characteristics of available datasets and assess how well they align. This evaluation is made easier by understanding a few key concepts:</p>
</section>
<section id="key-concepts-of-data-quality">
<h3>Key concepts of data quality<a class="headerlink" href="#key-concepts-of-data-quality" title="Link to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#key-concepts-of-data-quality'"><span>#</span></a></h3>
<ul class="simple">
<li><p><strong>Scope</strong> refers to the <strong>geographical, thematic, and temporal coverage</strong> of a dataset.
<em>Example: A land cover map of Ethiopia for the year 2020.</em></p></li>
<li><p><strong>Granularity</strong> describes how <strong>fine or coarse</strong> a dataset is.
<em>Example: Population figures at the municipal level are more granular than those at the regional level.</em></p></li>
<li><p><strong>Accuracy</strong> can refer to spatial, temporal, or thematic precision.
<em>Example: A satellite image with 10m resolution provides more spatial detail than one at 50m.</em></p></li>
<li><p><strong>Completeness</strong> reflects how much of the expected data is actually present.
<em>Example: A temperature dataset covering 8 regions is incomplete if data for some days or districts is missing.</em></p></li>
<li><p><strong>Temporal resolution</strong> indicates how frequently data is updated or captured — a key factor in disaster risk management.
<em>Example: A satellite with a 7-day revisit cycle is more likely to capture images close to the time of a flood or earthquake.</em></p></li>
</ul>
</section>
</section>
<section id="data-sovereignty-and-licensing">
<h2>Data sovereignty and licensing<a class="headerlink" href="#data-sovereignty-and-licensing" title="Link to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#data-sovereignty-and-licensing'"><span>#</span></a></h2>
<p>Data sovereignty is an increasingly important concept in the digital age, especially as a critical element of national and organizational security. In simple terms, data sovereignty refers to the extent to which data is subject to the laws, regulations, and governance of a particular country or jurisdiction.</p>
<p>This issue is particularly relevant in the context of disaster risk management, where data is an essential resource. Without timely and unrestricted access to key datasets, risk assessments and emergency responses can be delayed or compromised. Restrictions on access or usage—whether financial, legal, or technical—can have severe consequences during crises.</p>
<section id="licensing-and-usage-rights">
<h3>Licensing and Usage Rights<a class="headerlink" href="#licensing-and-usage-rights" title="Link to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#licensing-and-usage-rights'"><span>#</span></a></h3>
<p>Closely linked to data sovereignty is the issue of licensing. Understanding the license attached to a dataset is essential for determining how the data can be used. In some cases, the licensing terms may not align with national laws or may impose restrictions that limit the intended use of the data. This can trigger the need for a contingency plan to ensure operational continuity.</p>
<p>Licensing models can generally be grouped into two broad categories:</p>
<ul class="simple">
<li><p><strong>Open licenses</strong>, such as Creative Commons, which encourage sharing, reuse, and redistribution.</p></li>
<li><p><strong>Proprietary licenses</strong>, which often restrict the use, redistribution, or modification of data—especially for commercial or external applications.</p></li>
</ul>
</section>
<section id="software-dependencies-and-format-lock-in">
<h3>Software Dependencies and Format Lock-In<a class="headerlink" href="#software-dependencies-and-format-lock-in" title="Link to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#software-dependencies-and-format-lock-in'"><span>#</span></a></h3>
<p>A less obvious but significant form of data sovereignty risk stems from the use of proprietary software. Many proprietary systems rely on data formats that are not interoperable. As a result, accessing or fully using the data may require a valid license for the specific software that supports those formats. This dependency can limit flexibility and increase costs in the long run.</p>
</section>
<section id="incorporating-data-sovereignty-into-risk-planning">
<h3>Incorporating Data Sovereignty into Risk Planning<a class="headerlink" href="#incorporating-data-sovereignty-into-risk-planning" title="Link to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#incorporating-data-sovereignty-into-risk-planning'"><span>#</span></a></h3>
<p>Evaluating licensing and sovereignty-related constraints should be a standard part of any data management and risk planning strategy. Ensuring that critical data is accessible, legally usable, and interoperable is key to strengthening resilience and preparedness in disaster risk management.</p>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id2" role="note">
<span class="label"><span class="fn-bracket">[</span><a href="#id1" role="doc-backlink">1</a><span class="fn-bracket">]</span></span>
<p>Da Silva Mano, A. (2018). GIS in Sustainable Urban Planning and Management: Methodological demonstration for Chapter 18 - Utilising volunteered geographic information to assess resident’s flood evacuation shelters. Case study:Jakarta (pp. 307-321). Web publication/site, University of Twente, Faculty of Geo-Information Science and Earth Observation (ITC). https://www.itc.nl/urbangis/chapter-18/</p>
</aside>
</aside>
</section>
</section>
</section>
</div></div><aside class="hidden text-sm xl:block" id="right-sidebar">
<div class="sticky top-16 -mt-10 max-h-[calc(100vh-5rem)] overflow-y-auto pt-6 space-y-2"><p class="font-medium">On this page</p>
<ul>
<li><a :data-current="activeSection === '#what-is-spatial-data'" class="reference internal" href="#what-is-spatial-data">What is Spatial data?</a></li>
<li><a :data-current="activeSection === '#main-types-of-data'" class="reference internal" href="#main-types-of-data">Main types of data</a><ul>
<li><a :data-current="activeSection === '#digital-elevation-models-dem'" class="reference internal" href="#digital-elevation-models-dem">Digital Elevation Models (DEM)</a></li>
<li><a :data-current="activeSection === '#land-cover-maps'" class="reference internal" href="#land-cover-maps">Land Cover Maps</a></li>
<li><a :data-current="activeSection === '#land-cover-indices'" class="reference internal" href="#land-cover-indices">Land Cover Indices</a></li>
<li><a :data-current="activeSection === '#aerial-and-drone-photography'" class="reference internal" href="#aerial-and-drone-photography">Aerial and drone photography</a></li>
<li><a :data-current="activeSection === '#radar-data'" class="reference internal" href="#radar-data">Radar data</a></li>
<li><a :data-current="activeSection === '#lidar-data'" class="reference internal" href="#lidar-data">LiDAR data</a></li>
<li><a :data-current="activeSection === '#topographic-maps'" class="reference internal" href="#topographic-maps">Topographic maps</a></li>
<li><a :data-current="activeSection === '#volunteered-geographic-information-vgi'" class="reference internal" href="#volunteered-geographic-information-vgi">Volunteered geographic information (VGI)</a></li>
<li><a :data-current="activeSection === '#social-media-and-media-repositories-apis'" class="reference internal" href="#social-media-and-media-repositories-apis">Social media and media repositories APIs</a></li>
<li><a :data-current="activeSection === '#census-and-statistical-data'" class="reference internal" href="#census-and-statistical-data">Census and statistical data</a></li>
<li><a :data-current="activeSection === '#sensor-data'" class="reference internal" href="#sensor-data">Sensor data</a></li>
</ul>
</li>
<li><a :data-current="activeSection === '#sources-of-data'" class="reference internal" href="#sources-of-data">Sources of data</a></li>
<li><a :data-current="activeSection === '#data-quality'" class="reference internal" href="#data-quality">Data quality</a><ul>
<li><a :data-current="activeSection === '#assessing-data-quality'" class="reference internal" href="#assessing-data-quality">Assessing data quality</a></li>
<li><a :data-current="activeSection === '#key-concepts-of-data-quality'" class="reference internal" href="#key-concepts-of-data-quality">Key concepts of data quality</a></li>
</ul>
</li>
<li><a :data-current="activeSection === '#data-sovereignty-and-licensing'" class="reference internal" href="#data-sovereignty-and-licensing">Data sovereignty and licensing</a><ul>
<li><a :data-current="activeSection === '#licensing-and-usage-rights'" class="reference internal" href="#licensing-and-usage-rights">Licensing and Usage Rights</a></li>
<li><a :data-current="activeSection === '#software-dependencies-and-format-lock-in'" class="reference internal" href="#software-dependencies-and-format-lock-in">Software Dependencies and Format Lock-In</a></li>
<li><a :data-current="activeSection === '#incorporating-data-sovereignty-into-risk-planning'" class="reference internal" href="#incorporating-data-sovereignty-into-risk-planning">Incorporating Data Sovereignty into Risk Planning</a></li>
</ul>
</li>
</ul>
</div>
</aside>
</main>
</div>
</div><footer class="py-6 border-t border-border md:py-0">
<div class="container flex flex-col items-center justify-between gap-4 md:h-24 md:flex-row">
<div class="flex flex-col items-center gap-4 px-8 md:flex-row md:gap-2 md:px-0">
<p class="text-sm leading-loose text-center text-muted-foreground md:text-left">© University of Twente, 2025 Built with <a class="font-medium underline underline-offset-4" href="https://www.sphinx-doc.org" rel="noreferrer">Sphinx 7.3.7</a></p>
</div>
</div>
</footer>
</div>
<script src="_static/documentation_options.js?v=29a6c3e3"></script>
<script src="_static/doctools.js?v=9a2dae69"></script>
<script src="_static/sphinx_highlight.js?v=dc90522c"></script>
<script defer="defer" src="_static/theme.js?v=073f68d9"></script>
<script src="_static/design-tabs.js?v=f930bc37"></script>
<script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>